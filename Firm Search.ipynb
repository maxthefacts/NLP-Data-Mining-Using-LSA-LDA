{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load Tweets\n",
    "tweets = pd.read_pickle('selected_relevent_tweet.pkl')\n",
    "tweets = pd.DataFrame(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load Firmlist\n",
    "firmlist = pd.read_pickle('firmlist.pkl')\n",
    "# firmlist.drop_duplicates(subset = 'cusp_firm_name', inplace = True)\n",
    "# firmlist.reset_index(drop = True, inplace = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Brake into all firms that are more and less than 3 chareters long\n",
    "#If three chareters or less, look for token matches\n",
    "#If 4 charteres or more look for compelte string matches\n",
    "longfirms = firmlist[firmlist.length >= 4]\n",
    "shortfirms = firmlist[firmlist.length <= 3]\n",
    "longsearch = list(longfirms.search)\n",
    "shortsearch = list(shortfirms.search)\n",
    "\n",
    "shortfirms_strip = ['ear', 'ace', 'seg', 'les']\n",
    "longfirms_strip = ['money', 'chris', 'none', 'tcar', 'same', 'test', \"don'tknow\", \n",
    "                   'peter', 'various', 'unknown', 'charged', 'brooklyn']\n",
    "\n",
    "for word in shortfirms_strip:\n",
    "    shortsearch.remove(word)\n",
    "for word in longfirms_strip:\n",
    "    longsearch.remove(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create dataframe with all tweets in a single column along with its category\n",
    "all_tweets_matter = []\n",
    "all_cats_matter = []\n",
    "for cat in tweets:\n",
    "    for tweet in tweets[cat]:\n",
    "        if type(tweet) == str:\n",
    "            all_tweets_matter.append(tweet)\n",
    "            all_cats_matter.append(cat)\n",
    "\n",
    "test_tweets = pd.DataFrame(all_tweets_matter, columns= ['tweet'])\n",
    "test_tweets['cat'] = all_cats_matter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get rid of twitter handles, whitespace, retweet=RT, and make all lowercase\n",
    "#replace these cleaned tweets with the old tweets\n",
    "#drop duplicates\n",
    "strippedTweets = []\n",
    "for tweet in test_tweets.tweet:\n",
    "        #tempTweet = \" \".join(filter(lambda x:x[0]!='@', tweet.split()))\n",
    "        tempTweet = tweet.replace(' ', \"\")\n",
    "        tempTweet = tempTweet.replace('RT', \"\")\n",
    "        tempTweet = tempTweet.lower()\n",
    "        strippedTweets.append(tempTweet)\n",
    "test_tweets['striped_tweet'] = strippedTweets\n",
    "test_tweets.drop_duplicates(subset = 'striped_tweet', inplace = True)\n",
    "test_tweets.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create tokens out of tweets\n",
    "tokenized = []\n",
    "for tweet in test_tweets.tweet:\n",
    "    tokenized.append(tweet.split())\n",
    "test_tweets['tokens'] = tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'tweet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-32b67513cc30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtempmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshortsearch\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_tweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mjackpot_matches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtempmatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mjackpot_tweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mjackpot_cat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/maxfacts/anaconda/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2670\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2671\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2672\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'tweet'"
     ]
    }
   ],
   "source": [
    "#find tweets with firm names within the string\n",
    "jackpot_tweet = []\n",
    "jackpot_matches = []\n",
    "jackpot_cat = []\n",
    "for j in range(len(test_tweets.tokens)):\n",
    "    if len([i for i in shortsearch if i in test_tweets.tokens[j]]) > 0:\n",
    "        tempmatches = [i for i in shortsearch if i in test_tweets.tokens[j]]\n",
    "        jackpot_matches.append(set(tempmatches))\n",
    "        jackpot_tweet.append(tweets.tweet[j])\n",
    "        jackpot_cat.append(tweets.cat[j])\n",
    "\n",
    "for tweet in test_tweets.striped_tweet:\n",
    "    if any(x in tweet for x in longsearch):\n",
    "        tempmatches = {x for x in longsearch if x in tweet}\n",
    "        jackpot_matches.append(tempmatches)\n",
    "        jackpot_tweet.append(test_tweets.loc[tweets['striped_tweet'] == tweet].tweet.iloc[0])                      \n",
    "\n",
    "jackpot= pd.DataFrame(jackpot_tweet, columns = ['tweet'])\n",
    "jackpot['matches'] = jackpot_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#All in one function\n",
    "def tweetSearch(list_of_tweets):\n",
    "        \n",
    "    #Load Firmlist and brake into all firms that are more and less than 3 chareters long\n",
    "    #If three chareters or less, look for token matches\n",
    "    #If 4 charteres or more look for compelte string matches\n",
    "    #Remove manual words\n",
    "    firmlist = pd.read_pickle('firmlist.pkl')\n",
    "    \n",
    "    longfirms = firmlist[firmlist.length >= 4]\n",
    "    shortfirms = firmlist[firmlist.length <= 3]\n",
    "    longsearch = list(longfirms.search)\n",
    "    shortsearch = list(shortfirms.search)\n",
    "\n",
    "    shortfirms_strip = ['ear', 'ace', 'seg', 'les']\n",
    "    longfirms_strip = ['money', 'chris', 'none', 'tcar', 'same', 'test', \"don'tknow\", \n",
    "                       'peter', 'various', 'unknown', 'charged', 'brooklyn']\n",
    "\n",
    "    for word in shortfirms_strip:\n",
    "        shortsearch.remove(word)\n",
    "    for word in longfirms_strip:\n",
    "        longsearch.remove(word)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #get rid of twitter handles, whitespace, retweet=RT, and make all lowercase\n",
    "    #replace these cleaned tweets with the old tweets\n",
    "    #drop duplicates\n",
    "    tweets = pd.DataFrame(list_of_tweets, columns = ['tweet'])\n",
    "    strippedTweets = []   \n",
    "    for tweet in tweets.tweet:\n",
    "            #tempTweet = \" \".join(filter(lambda x:x[0]!='@', tweet.split()))\n",
    "            tempTweet = tweet.replace(' ', \"\")\n",
    "            tempTweet = tempTweet.replace('RT', \"\")\n",
    "            tempTweet = tempTweet.lower()\n",
    "            strippedTweets.append(tempTweet)\n",
    "    tweets['striped_tweet'] = strippedTweets\n",
    "    tweets.drop_duplicates(subset = 'striped_tweet', inplace = True)\n",
    "    tweets.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    #Create tokens out of tweets\n",
    "    tokenized = []\n",
    "    for tweet in tweets.tweet:\n",
    "        tokenized.append(tweet.split())\n",
    "    tweets['tokens'] = tokenized\n",
    "    \n",
    "    #find tweets with firm names within the string\n",
    "    jackpot_tweet = []\n",
    "    jackpot_matches = []\n",
    "    jackpot_cat = []\n",
    "    for j in range(len(tweets.tokens)):\n",
    "        if len([i for i in shortsearch if i in tweets.tokens[j]]) > 0:\n",
    "            tempmatches = [i for i in shortsearch if i in tweets.tokens[j]]\n",
    "            jackpot_matches.append(set(tempmatches))\n",
    "            jackpot_tweet.append(tweets.tweet[j])\n",
    "\n",
    "    for tweet in tweets.striped_tweet:\n",
    "        if any(x in tweet for x in longsearch):\n",
    "            tempmatches = {x for x in longsearch if x in tweet}\n",
    "            jackpot_matches.append(tempmatches)\n",
    "            jackpot_tweet.append(tweets.loc[tweets['striped_tweet'] == tweet].tweet.iloc[0])\n",
    "            \n",
    "    jackpot= pd.DataFrame(jackpot_tweet, columns = ['tweet'])\n",
    "    jackpot['matches'] = jackpot_matches\n",
    "    \n",
    "    now_its_a_list = []\n",
    "    for i in jackpot.matches:\n",
    "        if len (list(i)) > 0:\n",
    "            now_its_a_list.append(list(i))\n",
    "        else:\n",
    "            now_its_a_list.append(['no_company_listed'])\n",
    "    jackpot['matches'] = now_its_a_list\n",
    "    \n",
    "    return(jackpot, list(jackpot.tweet), list(jackpot.matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05234489838282267"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test to see how long the function takes\n",
    "start = time.time()\n",
    "x = tweetSearch(list(test_tweets.tweet))\n",
    "end = time.time()\n",
    "(end - start) / 60.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#just one tweet\n",
    "# def tweetSearch_for_one_tweet(tweets):\n",
    "    \n",
    "#     #Load Firmlist and brake into all firms that are more and less than 3 chareters long\n",
    "#     #If three chareters or less, look for token matches\n",
    "#     #If 4 charteres or more look for compelte string matches\n",
    "#     #Remove manual words\n",
    "#     firmlist = pd.read_pickle('firmlist.pkl')\n",
    "    \n",
    "#     longfirms = firmlist[firmlist.length >= 4]\n",
    "#     shortfirms = firmlist[firmlist.length <= 3]\n",
    "#     longsearch = list(longfirms.search)\n",
    "#     shortsearch = list(shortfirms.search)\n",
    "\n",
    "#     shortfirms_strip = ['ear', 'ace', 'seg', 'les']\n",
    "#     longfirms_strip = ['money', 'chris', 'none', 'tcar', 'same', 'test', \"don'tknow\", \n",
    "#                        'peter', 'various', 'unknown', 'charged', 'brooklyn']\n",
    "\n",
    "#     for word in shortfirms_strip:\n",
    "#         shortsearch.remove(word)\n",
    "#     for word in longfirms_strip:\n",
    "#         longsearch.remove(word)\n",
    "        \n",
    "#     #get rid of twitter handles, whitespace, retweet=RT, and make all lowercase\n",
    "#     #replace these cleaned tweets with the old tweets\n",
    "#     #drop duplicates\n",
    "    \n",
    "    \n",
    "    \n",
    "#     stripped_tweet = \" \".join(filter(lambda x:x[0]!='@', tweet.split()))\n",
    "#     stripped_tweet = stripped_tweet.replace(' ', \"\")\n",
    "#     stripped_tweet = stripped_tweet.replace('RT', \"\")\n",
    "#     stripped_tweet = stripped_tweet.lower()\n",
    "    \n",
    "#     #Create tokens out of tweets\n",
    "#     tokenized_tweet = tweet.split()\n",
    "    \n",
    "#     #find tweets with firm names within the string\n",
    "#     if len([i for i in shortsearch if i in tokenized_tweet]) > 0:\n",
    "#         matches = set([i for i in shortsearch if i in tokenized_tweet])\n",
    "#     #long search in stripped tweets\n",
    "#     elif any(x in tweet for x in longsearch):\n",
    "#         matches = {x for x in longsearch if x in tweet}\n",
    "        \n",
    "#     else:\n",
    "#         matches = ['no_company_mentioned']\n",
    "\n",
    "#     return(tweet, matches)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
